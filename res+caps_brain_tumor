{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1267593,"sourceType":"datasetVersion","datasetId":723383}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport h5py\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import class_weight\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras import backend as K","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-29T22:51:25.860574Z","iopub.execute_input":"2024-09-29T22:51:25.861224Z","iopub.status.idle":"2024-09-29T22:51:25.867922Z","shell.execute_reply.started":"2024-09-29T22:51:25.861174Z","shell.execute_reply":"2024-09-29T22:51:25.866948Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Global Parameters and Path","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\ntf.random.set_seed(42)\n\nDATA_PATH = '/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data'\n\nNUM_CLASSES = 4","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:51:25.869333Z","iopub.execute_input":"2024-09-29T22:51:25.869633Z","iopub.status.idle":"2024-09-29T22:51:25.879384Z","shell.execute_reply.started":"2024-09-29T22:51:25.869601Z","shell.execute_reply":"2024-09-29T22:51:25.878538Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Load HDF5 Files","metadata":{}},{"cell_type":"code","source":"def load_hdf5_slice(filepath):\n    \"\"\"\n    Load a single slice from the HDF5 file.\n    \"\"\"\n    with h5py.File(filepath, 'r') as h5_file:\n        image_data = h5_file['image'][:]\n        mask_data = h5_file['mask'][:]\n    return image_data, mask_data","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:51:25.881134Z","iopub.execute_input":"2024-09-29T22:51:25.881668Z","iopub.status.idle":"2024-09-29T22:51:25.889163Z","shell.execute_reply.started":"2024-09-29T22:51:25.881621Z","shell.execute_reply":"2024-09-29T22:51:25.888363Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Normalize and Resize Images","metadata":{}},{"cell_type":"code","source":"def normalize_image(image):\n    \"\"\"\n    Normalize the image intensities to the range [0, 1].\n    \"\"\"\n    min_val = np.min(image)\n    max_val = np.max(image)\n    if max_val - min_val == 0:\n        return image\n    else:\n        return (image - min_val) / (max_val - min_val)\n\ndef resize_image(image, target_size=(128, 128)):\n    \"\"\"\n    Resize the image to the target size.\n    \"\"\"\n    resized_image = np.zeros((target_size[0], target_size[1], image.shape[2]))\n    for i in range(image.shape[2]):\n        resized_image[..., i] = cv2.resize(image[..., i], target_size, interpolation=cv2.INTER_LINEAR)\n    return resized_image","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:51:25.890266Z","iopub.execute_input":"2024-09-29T22:51:25.890557Z","iopub.status.idle":"2024-09-29T22:51:25.898610Z","shell.execute_reply.started":"2024-09-29T22:51:25.890527Z","shell.execute_reply":"2024-09-29T22:51:25.897621Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess Slices","metadata":{}},{"cell_type":"code","source":"def preprocess_slice(image, mask, target_size=(128, 128)):\n    \"\"\"\n    Preprocess a single image and mask slice.\n    \"\"\"\n    # Normalize and resize image\n    image_normalized = normalize_image(image)\n    image_resized = resize_image(image_normalized, target_size)\n    \n    # Resize mask\n    mask_resized = resize_image(mask, target_size)\n    \n    # Convert mask to categorical labels\n    mask_resized = np.argmax(mask_resized, axis=-1)\n    return image_resized.astype(np.float32), mask_resized.astype(np.int32)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:51:25.901122Z","iopub.execute_input":"2024-09-29T22:51:25.901887Z","iopub.status.idle":"2024-09-29T22:51:25.911150Z","shell.execute_reply.started":"2024-09-29T22:51:25.901854Z","shell.execute_reply":"2024-09-29T22:51:25.910207Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Split Dataset into Train, Validation, and Test Sets","metadata":{}},{"cell_type":"code","source":"all_files = [os.path.join(DATA_PATH, f) for f in os.listdir(DATA_PATH) if f.endswith('.h5')]\ntotal_files = len(all_files)\nprint(f\"Total number of slices: {total_files}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:51:25.912452Z","iopub.execute_input":"2024-09-29T22:51:25.913011Z","iopub.status.idle":"2024-09-29T22:51:28.660279Z","shell.execute_reply.started":"2024-09-29T22:51:25.912968Z","shell.execute_reply":"2024-09-29T22:51:28.659351Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Total number of slices: 57195\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Shuffle and Split Files","metadata":{}},{"cell_type":"code","source":"train_val_files, test_files = train_test_split(all_files, test_size=0.2, random_state=42)\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.1, random_state=42)  # 10% of train_val for validation\n\nprint(f\"Training files: {len(train_files)}\")\nprint(f\"Validation files: {len(val_files)}\")\nprint(f\"Test files: {len(test_files)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:51:28.661649Z","iopub.execute_input":"2024-09-29T22:51:28.662044Z","iopub.status.idle":"2024-09-29T22:51:28.703052Z","shell.execute_reply.started":"2024-09-29T22:51:28.661998Z","shell.execute_reply":"2024-09-29T22:51:28.702143Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training files: 41180\nValidation files: 4576\nTest files: 11439\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Address Class Imbalance","metadata":{}},{"cell_type":"code","source":"def compute_class_weights(file_list):\n    all_labels = []\n    for file_path in file_list:\n        _, mask = load_hdf5_slice(file_path)\n        _, mask = preprocess_slice(_, mask)\n        labels, counts = np.unique(mask, return_counts=True)\n        all_labels.extend(labels.tolist())\n    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n    return dict(enumerate(class_weights))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:51:28.704092Z","iopub.execute_input":"2024-09-29T22:51:28.704391Z","iopub.status.idle":"2024-09-29T22:51:28.710509Z","shell.execute_reply.started":"2024-09-29T22:51:28.704351Z","shell.execute_reply":"2024-09-29T22:51:28.709533Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class_weights = compute_class_weights(train_files)\nprint(f\"Class weights: {class_weights}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T22:51:28.711676Z","iopub.execute_input":"2024-09-29T22:51:28.712027Z","iopub.status.idle":"2024-09-29T23:06:01.636965Z","shell.execute_reply.started":"2024-09-29T22:51:28.711985Z","shell.execute_reply":"2024-09-29T23:06:01.635971Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Class weights: {0: 0.5555771410069613, 1: 1.326222634436651, 2: 2.2419075616527846}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"!pip install -q albumentations==1.0.3\n\nimport albumentations as A\nfrom albumentations.core.composition import Compose","metadata":{"execution":{"iopub.status.busy":"2024-09-29T23:06:01.640291Z","iopub.execute_input":"2024-09-29T23:06:01.640641Z","iopub.status.idle":"2024-09-29T23:06:14.907495Z","shell.execute_reply.started":"2024-09-29T23:06:01.640608Z","shell.execute_reply":"2024-09-29T23:06:14.906576Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}]},{"cell_type":"code","source":"augmentation_pipeline = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(limit=20, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.5),\n    A.RandomBrightnessContrast(p=0.5),\n], additional_targets={'mask': 'mask'})","metadata":{"execution":{"iopub.status.busy":"2024-09-29T23:06:14.908967Z","iopub.execute_input":"2024-09-29T23:06:14.910020Z","iopub.status.idle":"2024-09-29T23:06:14.916230Z","shell.execute_reply.started":"2024-09-29T23:06:14.909968Z","shell.execute_reply":"2024-09-29T23:06:14.915343Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Custom Data Generator","metadata":{}},{"cell_type":"code","source":"class DataGenerator(Sequence):\n    def __init__(self, file_list, batch_size=8, target_size=(128, 128), augment=False, shuffle=True):\n        self.file_list = file_list\n        self.batch_size = batch_size\n        self.target_size = target_size\n        self.augment = augment\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(self.file_list))\n        self.on_epoch_end()\n        \n    def __len__(self):\n        return int(np.ceil(len(self.file_list) / self.batch_size))\n    \n    def __getitem__(self, index):\n        # Generate indexes of the batch\n        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        \n        # Get the files for the current batch\n        batch_files = [self.file_list[k] for k in batch_indexes]\n        \n        # Generate data\n        X, y = self.__data_generation(batch_files)\n        return X, y\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n    \n    def __data_generation(self, batch_files):\n        X = []\n        y = []\n        for file_path in batch_files:\n            # Load image and mask\n            image, mask = load_hdf5_slice(file_path)\n            image, mask = preprocess_slice(image, mask, target_size=self.target_size)\n            \n            # Apply augmentation if enabled\n            if self.augment:\n                augmented = augmentation_pipeline(image=image, mask=mask)\n                image = augmented['image']\n                mask = augmented['mask']\n            \n            # Convert mask to categorical labels (classification)\n            labels, counts = np.unique(mask, return_counts=True)\n            # Assume the class with the highest pixel count is the label\n            label = labels[np.argmax(counts)]\n            X.append(image)\n            y.append(label)\n        \n        X = np.array(X)\n        y = np.array(y)\n        y = tf.keras.utils.to_categorical(y, num_classes=NUM_CLASSES)\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2024-09-29T23:44:16.129947Z","iopub.execute_input":"2024-09-29T23:44:16.130692Z","iopub.status.idle":"2024-09-29T23:44:16.143068Z","shell.execute_reply.started":"2024-09-29T23:44:16.130651Z","shell.execute_reply":"2024-09-29T23:44:16.141989Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# ResNet + CapsNet Model","metadata":{}},{"cell_type":"markdown","source":"## Capsule Layer","metadata":{}},{"cell_type":"code","source":"def squash(vectors, axis=-1):\n    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=axis, keepdims=True)\n    scale = s_squared_norm / (1 + s_squared_norm)\n    return scale * vectors / tf.sqrt(s_squared_norm + K.epsilon())\n\nclass CapsuleLayer(layers.Layer):\n    def __init__(self, num_capsules, dim_capsules, routings=3, **kwargs):\n        super(CapsuleLayer, self).__init__(**kwargs)\n        self.num_capsules = num_capsules\n        self.dim_capsules = dim_capsules\n        self.routings = routings\n\n    def build(self, input_shape):\n        self.input_num_capsules = input_shape[1]\n        self.input_dim_capsules = input_shape[2]\n        \n        self.W = self.add_weight(shape=(self.input_num_capsules, self.num_capsules, self.input_dim_capsules, self.dim_capsules),\n                                 initializer='glorot_uniform',\n                                 name='W')\n        super(CapsuleLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        inputs_expand = tf.expand_dims(inputs, 2)\n        inputs_expand = tf.expand_dims(inputs_expand, -1)\n\n        W_expand = tf.expand_dims(self.W, 0)\n        \n        u_hat = tf.matmul(W_expand, inputs_expand)\n        u_hat = tf.squeeze(u_hat, axis=-2)\n        \n        b = tf.zeros(shape=[tf.shape(inputs)[0], self.input_num_capsules, self.num_capsules])\n        for i in range(self.routings):\n            c = tf.nn.softmax(b, axis=2)\n            c = tf.expand_dims(c, -1)\n            s = tf.reduce_sum(c * u_hat, axis=1)\n            v = squash(s)\n            if i < self.routings - 1:\n                v_tiled = tf.expand_dims(v, 1) \n                b += tf.reduce_sum(u_hat * v_tiled, axis=-1)\n        return v","metadata":{"execution":{"iopub.status.busy":"2024-09-29T23:44:21.137592Z","iopub.execute_input":"2024-09-29T23:44:21.137975Z","iopub.status.idle":"2024-09-29T23:44:21.150275Z","shell.execute_reply.started":"2024-09-29T23:44:21.137940Z","shell.execute_reply":"2024-09-29T23:44:21.149337Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def margin_loss(y_true, y_pred):\n    m_plus = 0.9\n    m_minus = 0.1\n    lambda_val = 0.5\n\n    L = y_true * tf.square(tf.maximum(0., m_plus - y_pred)) + \\\n        lambda_val * (1 - y_true) * tf.square(tf.maximum(0., y_pred - m_minus))\n\n    return tf.reduce_mean(tf.reduce_sum(L, axis=1))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T23:44:25.743488Z","iopub.execute_input":"2024-09-29T23:44:25.744134Z","iopub.status.idle":"2024-09-29T23:44:25.749748Z","shell.execute_reply.started":"2024-09-29T23:44:25.744091Z","shell.execute_reply":"2024-09-29T23:44:25.748804Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Model Build","metadata":{}},{"cell_type":"code","source":"def create_resnet_capsnet_model(input_shape=(128, 128, 4)):\n    inputs = layers.Input(shape=input_shape)\n    \n    # ResNet50 as feature extractor\n    base_model = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n    \n    x = base_model.output\n    x = layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='valid', activation='relu')(x)\n    \n    # Create Primary Capsules\n    NUM_PRIMARY_CAPSULES = 32\n    DIM_PRIMARY_CAPSULES = 8\n    x = layers.Conv2D(filters=NUM_PRIMARY_CAPSULES * DIM_PRIMARY_CAPSULES, kernel_size=3, strides=2, padding='valid')(x)\n    x = layers.Reshape((-1, DIM_PRIMARY_CAPSULES))(x)\n    x = layers.Lambda(squash)(x)\n    \n    # Capsule Layer\n    capsule = CapsuleLayer(num_capsules=NUM_CLASSES, dim_capsules=16, routings=3, name='capsule_layer')(x)\n\n    outputs = Length(name='capsnet_output')(capsule)\n    \n    model = models.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-29T23:44:32.024101Z","iopub.execute_input":"2024-09-29T23:44:32.024511Z","iopub.status.idle":"2024-09-29T23:44:32.032834Z","shell.execute_reply.started":"2024-09-29T23:44:32.024472Z","shell.execute_reply":"2024-09-29T23:44:32.031727Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Compile Model","metadata":{}},{"cell_type":"code","source":"model = create_resnet_capsnet_model(input_shape=(128, 128, 4))\noptimizer = optimizers.Adam(learning_rate=1e-4)\nmodel.compile(optimizer=optimizer, loss=margin_loss, metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-29T23:44:36.174622Z","iopub.execute_input":"2024-09-29T23:44:36.175028Z","iopub.status.idle":"2024-09-29T23:44:37.182480Z","shell.execute_reply.started":"2024-09-29T23:44:36.174990Z","shell.execute_reply":"2024-09-29T23:44:37.180958Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_resnet_capsnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mmargin_loss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","Cell \u001b[0;32mIn[24], line 5\u001b[0m, in \u001b[0;36mcreate_resnet_capsnet_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ResNet50 as feature extractor\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/applications/resnet.py:406\u001b[0m, in \u001b[0;36mResNet50\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    403\u001b[0m     x \u001b[38;5;241m=\u001b[39m stack_residual_blocks_v1(x, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m6\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stack_residual_blocks_v1(x, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m3\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/applications/resnet.py:210\u001b[0m, in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    203\u001b[0m         file_hash \u001b[38;5;241m=\u001b[39m WEIGHTS_HASHES[model_name][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    204\u001b[0m     weights_path \u001b[38;5;241m=\u001b[39m file_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[1;32m    205\u001b[0m         file_name,\n\u001b[1;32m    206\u001b[0m         BASE_WEIGHTS_PATH \u001b[38;5;241m+\u001b[39m file_name,\n\u001b[1;32m    207\u001b[0m         cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    208\u001b[0m         file_hash\u001b[38;5;241m=\u001b[39mfile_hash,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[0;32m--> 210\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(weights)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/common/variables.py:226\u001b[0m, in \u001b[0;36mKerasVariable.assign\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    224\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_tensor(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_equal(value\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the target variable and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe shape of the target value in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`variable.assign(value)` must match. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: value.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_stateless_scope():\n\u001b[1;32m    235\u001b[0m     scope \u001b[38;5;241m=\u001b[39m get_stateless_scope()\n","\u001b[0;31mValueError\u001b[0m: The shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(7, 7, 4, 64), Received: value.shape=(7, 7, 3, 64). Target variable: <KerasVariable shape=(7, 7, 4, 64), dtype=float32, path=conv1_conv/kernel>"],"ename":"ValueError","evalue":"The shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(7, 7, 4, 64), Received: value.shape=(7, 7, 3, 64). Target variable: <KerasVariable shape=(7, 7, 4, 64), dtype=float32, path=conv1_conv/kernel>","output_type":"error"}]},{"cell_type":"markdown","source":"# K-Fold Cross-Validation","metadata":{}},{"cell_type":"code","source":"k = 5\nkf = KFold(n_splits=k, shuffle=True, random_state=42)\nepochs = 25\nbatch_size = 8\n\ntrain_val_files = train_files + val_files  # 80% of data\ntest_files = test_files  # 20% of data\n\nfold_no = 1\nfor train_index, val_index in kf.split(train_val_files):\n    print(f'\\nTraining for fold {fold_no} ...')\n    \n    train_files_fold = [train_val_files[i] for i in train_index]\n    val_files_fold = [train_val_files[i] for i in val_index]\n    \n    class_weights_fold = compute_class_weights(train_files_fold)\n    \n    train_generator = DataGenerator(train_files_fold, batch_size=batch_size, augment=True, shuffle=True)\n    val_generator = DataGenerator(val_files_fold, batch_size=batch_size, augment=False, shuffle=False)\n    \n    checkpoint_filepath = f'best_model_fold_{fold_no}.keras'\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_best_only=True,\n        monitor='val_accuracy',\n        mode='max'\n    )\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n    \n    history = model.fit(\n        train_generator,\n        steps_per_epoch=len(train_generator),\n        validation_data=val_generator,\n        validation_steps=len(val_generator),\n        epochs=epochs,\n        callbacks=[model_checkpoint, early_stopping, reduce_lr],\n        class_weight=class_weights_fold,\n        verbose=1\n    )\n    \n    model.load_weights(checkpoint_filepath)\n    \n    val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n    print(f'Fold {fold_no} Validation Accuracy: {val_accuracy}')\n    \n    fold_no += 1","metadata":{"execution":{"iopub.status.busy":"2024-09-29T23:06:20.611124Z","iopub.execute_input":"2024-09-29T23:06:20.611547Z","iopub.status.idle":"2024-09-29T23:16:22.539529Z","shell.execute_reply.started":"2024-09-29T23:06:20.611504Z","shell.execute_reply":"2024-09-29T23:16:22.537908Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\nTraining for fold 1 ...\nEpoch 1/25\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_filepath)\n\u001b[1;32m     44\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(val_generator, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mcreate_resnet_capsnet_model.<locals>.<lambda>\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mReshape((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m512\u001b[39m))(x)\n\u001b[1;32m     19\u001b[0m capsule \u001b[38;5;241m=\u001b[39m CapsuleLayer(num_capsules\u001b[38;5;241m=\u001b[39mNUM_CLASSES, dim_capsules\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, routings\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapsule_layer\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mLambda(\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m z: tf\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     22\u001b[0m     output_shape\u001b[38;5;241m=\u001b[39m(NUM_CLASSES,)\n\u001b[1;32m     23\u001b[0m )(capsule)\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mInvalid reduction dimension 2 for input with 2 dimensions. for '{{node functional_1_1/lambda_1/Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](functional_1_1/lambda_1/Square, functional_1_1/lambda_1/Sum/reduction_indices)' with input shapes: [?,16], [] and with computed input tensors: input[1] = <2>.\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=tf.Tensor(shape=(None, 16), dtype=float32)\n  • mask=None\n  • training=True"],"ename":"ValueError","evalue":"Exception encountered when calling Lambda.call().\n\n\u001b[1mInvalid reduction dimension 2 for input with 2 dimensions. for '{{node functional_1_1/lambda_1/Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](functional_1_1/lambda_1/Square, functional_1_1/lambda_1/Sum/reduction_indices)' with input shapes: [?,16], [] and with computed input tensors: input[1] = <2>.\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=tf.Tensor(shape=(None, 16), dtype=float32)\n  • mask=None\n  • training=True","output_type":"error"}]}]}